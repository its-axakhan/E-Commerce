{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd627744",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T12:57:44.897281Z",
     "iopub.status.busy": "2025-11-24T12:57:44.897129Z",
     "iopub.status.idle": "2025-11-24T12:57:45.367409Z",
     "shell.execute_reply": "2025-11-24T12:57:45.366974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-24 04:57:44,901 [INFO] Starting US ESCI pipeline...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"pipeline.log\"),\n",
    "        logging.StreamHandler(sys.stdout),  # still captured in notebook\n",
    "    ],\n",
    ")\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "log.info(\"Starting US ESCI pipeline...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e98849e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T12:57:45.369041Z",
     "iopub.status.busy": "2025-11-24T12:57:45.368845Z",
     "iopub.status.idle": "2025-11-24T12:57:58.412460Z",
     "shell.execute_reply": "2025-11-24T12:57:58.411358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SYSTEM INFO ===\n",
      "Machine: x86_64\n",
      "Processor: x86_64\n",
      "Python: 3.11.11\n",
      "\n",
      "=== CPU ===\n",
      "Cores: 64\n",
      "CPU Usage: 1.0 %\n",
      "\n",
      "=== RAM ===\n",
      "Total: 270.29 GB\n",
      "Used: 4.0 %\n",
      "\n",
      "=== DISK ===\n",
      "Total: 75.13 GB\n",
      "Used: 65.09 GB\n",
      "\n",
      "=== GPU ===\n",
      "CUDA Available: True\n",
      "GPU: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import platform, psutil, shutil, torch\n",
    "\n",
    "print(\"=== SYSTEM INFO ===\")\n",
    "print(\"Machine:\", platform.machine())\n",
    "print(\"Processor:\", platform.processor())\n",
    "print(\"Python:\", platform.python_version())\n",
    "\n",
    "# CPU\n",
    "print(\"\\n=== CPU ===\")\n",
    "print(\"Cores:\", psutil.cpu_count())\n",
    "print(\"CPU Usage:\", psutil.cpu_percent(), \"%\")\n",
    "\n",
    "# RAM\n",
    "print(\"\\n=== RAM ===\")\n",
    "ram = psutil.virtual_memory()\n",
    "print(\"Total:\", round(ram.total/1e9, 2), \"GB\")\n",
    "print(\"Used:\", ram.percent, \"%\")\n",
    "\n",
    "# Disk\n",
    "print(\"\\n=== DISK ===\")\n",
    "disk = shutil.disk_usage(\"/\")\n",
    "print(\"Total:\", round(disk.total/1e9, 2), \"GB\")\n",
    "print(\"Used:\", round(disk.used/1e9, 2), \"GB\")\n",
    "\n",
    "# GPU\n",
    "print(\"\\n=== GPU ===\")\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f826fb8d",
   "metadata": {},
   "source": [
    "# 0. Imports, Install (if needed) & Config\n",
    "\n",
    "\n",
    " If you don't need installs, you can comment this cell out.\n",
    " Make sure you're using the same Python env as your previous notebook.\n",
    "\n",
    " !pip install pandas pyarrow numpy fastparquet scikit-learn rank-bm25 lightgbm datasets transformers sentencepiece einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de26e845-0ed9-405c-ae2e-d3040b63a385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T12:57:58.414922Z",
     "iopub.status.busy": "2025-11-24T12:57:58.414532Z",
     "iopub.status.idle": "2025-11-24T12:57:58.419791Z",
     "shell.execute_reply": "2025-11-24T12:57:58.419174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /opt/modules/devel/python/3.11.11/bin/python\n",
      "\n",
      "Filtered sys.path (no python3.8 entries):\n",
      "  \n",
      "  /mnt/ceph/bibi8250/E-commerce\n",
      "  /opt/modules/devel/python/3.11.11/lib/python311.zip\n",
      "  /opt/modules/devel/python/3.11.11/lib/python3.11\n",
      "  /opt/modules/devel/python/3.11.11/lib/python3.11/lib-dynload\n",
      "  /mnt/ceph/bibi8250/.local/lib/python3.11/site-packages\n",
      "  /opt/modules/devel/python/3.11.11/lib/python3.11/site-packages\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "sys.path = [p for p in sys.path if \"python3.8\" not in p]\n",
    "\n",
    "print(\"\\nFiltered sys.path (no python3.8 entries):\")\n",
    "for p in sys.path:\n",
    "    print(\" \", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f7bf81-17e5-4d63-a9e5-4cb4a99d83fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T12:57:58.421566Z",
     "iopub.status.busy": "2025-11-24T12:57:58.421187Z",
     "iopub.status.idle": "2025-11-24T12:58:21.750056Z",
     "shell.execute_reply": "2025-11-24T12:58:21.749149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: /opt/modules/devel/python/3.11.11/bin/python\n",
      "2025-11-24 04:58:10,868 [INFO] TensorFlow version 2.20.0 available.\n",
      "\n",
      "ALL CODE-CRITICAL IMPORTS LOADED OK\n",
      "\n",
      "numpy: 1.26.4\n",
      "pandas: 2.3.3\n",
      "sklearn: 1.7.2\n",
      "rank_bm25: <class 'rank_bm25.BM25Okapi'>\n",
      "lightgbm: 4.6.0\n",
      "torch: 2.5.1+cu121\n",
      "pyarrow: 22.0.0\n",
      "datasets: 4.4.1\n",
      "transformers: 4.57.1\n",
      "huggingface_hub: 0.36.0\n",
      "tokenizers: 0.22.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python executable:\", sys.executable)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from rank_bm25 import BM25Okapi\n",
    "import lightgbm as lgb\n",
    "import torch\n",
    "import pyarrow\n",
    "import datasets\n",
    "import transformers\n",
    "import huggingface_hub\n",
    "import tokenizers\n",
    "\n",
    "print(\"\\nALL CODE-CRITICAL IMPORTS LOADED OK\\n\")\n",
    "\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"rank_bm25:\", BM25Okapi)\n",
    "print(\"lightgbm:\", lgb.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"pyarrow:\", pyarrow.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
    "print(\"tokenizers:\", tokenizers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3936fe33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T12:58:21.752813Z",
     "iopub.status.busy": "2025-11-24T12:58:21.751982Z",
     "iopub.status.idle": "2025-11-24T12:58:58.071772Z",
     "shell.execute_reply": "2025-11-24T12:58:58.070902Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-24 04:58:40.065383: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import lightgbm as lgb\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# ---- CONFIG ----\n",
    "DATA_ROOT    = Path(\"esci_pipeline/data\")\n",
    "PATH_TRAIN   = DATA_ROOT / \"esci_train.parquet\"\n",
    "PATH_TEST    = DATA_ROOT / \"esci_test.parquet\"\n",
    "\n",
    "LOCALE       = \"us\"  # change later for \"es\", \"jp\", etc.\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Models\n",
    "GTE_MODEL_NAME  = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "XENC_MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "\n",
    "# ---- PERSISTENCE CONFIG ----\n",
    "ARTIFACT_DIR = Path(\"esci_pipeline/artifacts_us\")\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PATH_TRAIN_PREP = ARTIFACT_DIR / \"train_df_prepared.parquet\"\n",
    "PATH_VALID_PREP = ARTIFACT_DIR / \"valid_df_prepared.parquet\"\n",
    "PATH_PROD_EMBS  = ARTIFACT_DIR / \"prod_embs.npy\"\n",
    "PATH_PROD_IDS   = ARTIFACT_DIR / \"product_ids.npy\"\n",
    "PATH_BM25_PKL   = ARTIFACT_DIR / \"bm25_corpus.pkl\"\n",
    "PATH_LGB_MODEL  = ARTIFACT_DIR / \"ltr_model_us.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0705d784",
   "metadata": {},
   "source": [
    "# 1. Load & Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ae92450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T12:58:58.073992Z",
     "iopub.status.busy": "2025-11-24T12:58:58.073561Z",
     "iopub.status.idle": "2025-11-24T12:59:33.566268Z",
     "shell.execute_reply": "2025-11-24T12:59:33.565257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shapes: (2027874, 14) (652490, 14)\n",
      "Locales train:\n",
      " product_locale\n",
      "us    1420372\n",
      "jp     333112\n",
      "es     274390\n",
      "Name: count, dtype: int64\n",
      "Cleaned shapes: (1420372, 14) (434234, 14)\n",
      "Relevance distribution (train):\n",
      "relevance\n",
      "0    122273\n",
      "1     29713\n",
      "2    280324\n",
      "3    988062\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1.1 Load ESCI train/test\n",
    "esci_train = pd.read_parquet(PATH_TRAIN)\n",
    "esci_test  = pd.read_parquet(PATH_TEST)\n",
    "\n",
    "print(\"Raw shapes:\", esci_train.shape, esci_test.shape)\n",
    "print(\"Locales train:\\n\", esci_train[\"product_locale\"].value_counts())\n",
    "\n",
    "# 1.2 Filter to locale (US for now)\n",
    "esci_train = esci_train[esci_train[\"product_locale\"] == LOCALE].copy()\n",
    "esci_test  = esci_test[esci_test[\"product_locale\"] == LOCALE].copy()\n",
    "\n",
    "# 1.3 Drop rows with missing query/title\n",
    "esci_train = esci_train.dropna(subset=[\"query\", \"product_title\"])\n",
    "esci_test  = esci_test.dropna(subset=[\"query\", \"product_title\"])\n",
    "\n",
    "# 1.4 Fill remaining text NaNs\n",
    "text_cols = [\n",
    "    \"product_description\",\n",
    "    \"product_bullet_point\",\n",
    "    \"product_brand\",\n",
    "    \"product_color\",\n",
    "    \"product_text\",\n",
    "]\n",
    "\n",
    "for col in text_cols:\n",
    "    esci_train[col] = esci_train[col].fillna(\"\")\n",
    "    esci_test[col]  = esci_test[col].fillna(\"\")\n",
    "\n",
    "print(\"Cleaned shapes:\", esci_train.shape, esci_test.shape)\n",
    "\n",
    "# 1.5 Map ESCI labels to numeric relevance 0–3\n",
    "label2rel = {\n",
    "    \"Irrelevant\": 0,\n",
    "    \"Complement\": 1,\n",
    "    \"Substitute\": 2,\n",
    "    \"Exact\": 3,\n",
    "}\n",
    "\n",
    "esci_train[\"relevance\"] = esci_train[\"esci_label\"].map(label2rel).astype(\"int8\")\n",
    "esci_test[\"relevance\"]  = esci_test[\"esci_label\"].map(label2rel).astype(\"int8\")\n",
    "\n",
    "print(\"Relevance distribution (train):\")\n",
    "print(esci_train[\"relevance\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8c93987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T12:59:33.569165Z",
     "iopub.status.busy": "2025-11-24T12:59:33.568700Z",
     "iopub.status.idle": "2025-11-24T13:00:03.825008Z",
     "shell.execute_reply": "2025-11-24T13:00:03.824011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full US label counts:\n",
      " relevance\n",
      "0    122273\n",
      "1     29713\n",
      "2    280324\n",
      "3    988062\n",
      "Name: count, dtype: int64\n",
      "Ideal per-class target: 75000\n",
      "⚠ Smallest class only has 29713 examples; using n_per_class=29713 (total=118852) instead of full 10,000.\n",
      "Using n_per_class = 29713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952182/4081392387.py:32: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(n=n_per_class, random_state=RANDOM_STATE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced subset shape: (118852, 15)\n",
      "relevance\n",
      "0    29713\n",
      "1    29713\n",
      "2    29713\n",
      "3    29713\n",
      "Name: count, dtype: int64\n",
      "Saved balanced subset to: esci_pipeline/data/esci_us_balanced_300k.parquet\n"
     ]
    }
   ],
   "source": [
    "# ---- Balanced 300k subset of US ESCI (by relevance) ----\n",
    "DESIRED_TOTAL = 300_000\n",
    "LABEL_COL = \"relevance\"\n",
    "\n",
    "# Check class distribution\n",
    "label_counts = esci_train[LABEL_COL].value_counts().sort_index()\n",
    "print(\"Full US label counts:\\n\", label_counts)\n",
    "\n",
    "num_classes = label_counts.shape[0]\n",
    "\n",
    "# Ideal per-class target (for 300k total)\n",
    "ideal_per_class = DESIRED_TOTAL // num_classes\n",
    "print(\"Ideal per-class target:\", ideal_per_class)\n",
    "\n",
    "# But make sure we don't ask more than the smallest class has\n",
    "min_available = label_counts.min()\n",
    "n_per_class = min(ideal_per_class, min_available)\n",
    "\n",
    "if n_per_class < ideal_per_class:\n",
    "    print(\n",
    "        f\"⚠ Smallest class only has {min_available} examples; \"\n",
    "        f\"using n_per_class={n_per_class} (total={n_per_class * num_classes}) \"\n",
    "        f\"instead of full 10,000.\"\n",
    "    )\n",
    "\n",
    "print(\"Using n_per_class =\", n_per_class)\n",
    "\n",
    "# Sample a balanced subset\n",
    "esci_train_balanced = (\n",
    "    esci_train\n",
    "    .groupby(LABEL_COL, group_keys=False)\n",
    "    .apply(lambda g: g.sample(n=n_per_class, random_state=RANDOM_STATE))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Balanced subset shape:\", esci_train_balanced.shape)\n",
    "print(esci_train_balanced[LABEL_COL].value_counts().sort_index())\n",
    "\n",
    "# Optionally save for translation / reuse\n",
    "BALANCED_PATH = DATA_ROOT / \"esci_us_balanced_300k.parquet\"\n",
    "esci_train_balanced.to_parquet(BALANCED_PATH, index=False)\n",
    "print(\"Saved balanced subset to:\", BALANCED_PATH)\n",
    "\n",
    "esci_train = esci_train_balanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1618722a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T13:00:03.827001Z",
     "iopub.status.busy": "2025-11-24T13:00:03.826814Z",
     "iopub.status.idle": "2025-11-24T13:00:17.435607Z",
     "shell.execute_reply": "2025-11-24T13:00:17.434469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full US TEST label counts:\n",
      " relevance\n",
      "0     43505\n",
      "1     11147\n",
      "2     97163\n",
      "3    282419\n",
      "Name: count, dtype: int64\n",
      "Ideal per-class target (test): 15000\n",
      "⚠ Smallest class only has 11147 examples; using n_per_class_test=11147 (total=44588) instead of 60000.\n",
      "Using n_per_class_test = 11147\n",
      "Balanced TEST subset shape: (44588, 15)\n",
      "relevance\n",
      "0    11147\n",
      "1    11147\n",
      "2    11147\n",
      "3    11147\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3952182/3801891425.py:29: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: g.sample(n=n_per_class_test, random_state=RANDOM_STATE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved balanced 10k US TEST dataset to: esci_pipeline/data/us_test_balanced_60k.parquet\n"
     ]
    }
   ],
   "source": [
    "# ---- Balanced 10k subset of US ESCI TEST (by relevance) ----\n",
    "DESIRED_TEST = 60_000\n",
    "LABEL_COL_TEST = \"relevance\"\n",
    "\n",
    "# Check test distribution\n",
    "label_counts_test = esci_test[LABEL_COL_TEST].value_counts().sort_index()\n",
    "print(\"Full US TEST label counts:\\n\", label_counts_test)\n",
    "\n",
    "num_classes_test = label_counts_test.shape[0]\n",
    "ideal_per_class_test = DESIRED_TEST // num_classes_test\n",
    "print(\"Ideal per-class target (test):\", ideal_per_class_test)\n",
    "\n",
    "min_available_test = label_counts_test.min()\n",
    "n_per_class_test = min(ideal_per_class_test, min_available_test)\n",
    "\n",
    "if n_per_class_test < ideal_per_class_test:\n",
    "    print(\n",
    "        f\"⚠ Smallest class only has {min_available_test} examples; \"\n",
    "        f\"using n_per_class_test={n_per_class_test} \"\n",
    "        f\"(total={n_per_class_test * num_classes_test}) instead of {DESIRED_TEST}.\"\n",
    "    )\n",
    "\n",
    "print(\"Using n_per_class_test =\", n_per_class_test)\n",
    "\n",
    "# Create balanced test dataset\n",
    "esci_test_balanced = (\n",
    "    esci_test\n",
    "    .groupby(LABEL_COL_TEST, group_keys=False)\n",
    "    .apply(lambda g: g.sample(n=n_per_class_test, random_state=RANDOM_STATE))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Balanced TEST subset shape:\", esci_test_balanced.shape)\n",
    "print(esci_test_balanced[LABEL_COL_TEST].value_counts().sort_index())\n",
    "\n",
    "# Save the FINAL 10k parquest (no translation)\n",
    "test_path = DATA_ROOT / \"us_test_balanced_60k.parquet\"\n",
    "esci_test_balanced.to_parquet(test_path, index=False)\n",
    "\n",
    "print(\"Saved balanced 10k US TEST dataset to:\", test_path)\n",
    "\n",
    "esci_test = pd.read_parquet(DATA_ROOT / \"us_test_balanced_60k.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad80af",
   "metadata": {},
   "source": [
    "# 2. Unified Text + Explicit Context Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69509aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T13:00:17.438395Z",
     "iopub.status.busy": "2025-11-24T13:00:17.438221Z",
     "iopub.status.idle": "2025-11-24T13:00:22.597797Z",
     "shell.execute_reply": "2025-11-24T13:00:22.597042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>query</th>\n",
       "      <th>query_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_locale</th>\n",
       "      <th>esci_label</th>\n",
       "      <th>small_version</th>\n",
       "      <th>large_version</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_description</th>\n",
       "      <th>...</th>\n",
       "      <th>ctx_title_len</th>\n",
       "      <th>ctx_desc_len</th>\n",
       "      <th>ctx_bullet_len</th>\n",
       "      <th>ctx_brand_in_title</th>\n",
       "      <th>ctx_color_in_title</th>\n",
       "      <th>prod_count</th>\n",
       "      <th>prod_mean_rel</th>\n",
       "      <th>prod_max_rel</th>\n",
       "      <th>query_count</th>\n",
       "      <th>query_mean_rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>193260</td>\n",
       "      <td>amazon.com/code activate</td>\n",
       "      <td>8677</td>\n",
       "      <td>B0066TUXU6</td>\n",
       "      <td>us</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Hulu</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>264260</td>\n",
       "      <td>babygirl anklet</td>\n",
       "      <td>12307</td>\n",
       "      <td>B07663TBC4</td>\n",
       "      <td>us</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Goldenchen Fashion Jewelry 925 Silver Plated A...</td>\n",
       "      <td>Goldenchen is a fashion jewelry leader that eq...</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1311389</td>\n",
       "      <td>max japan</td>\n",
       "      <td>66517</td>\n",
       "      <td>B06Y1CBVKT</td>\n",
       "      <td>us</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Max Vaimo HD-11FLK Flat Clinch Stapler with 3 ...</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1632982</td>\n",
       "      <td>pressure washer oil</td>\n",
       "      <td>83198</td>\n",
       "      <td>B004S67CUS</td>\n",
       "      <td>us</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>SIMPSON Cleaning Pressure Washer Pump Guard</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300511</td>\n",
       "      <td>beach hats for women</td>\n",
       "      <td>14164</td>\n",
       "      <td>B072BJPTKJ</td>\n",
       "      <td>us</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CUPSHE Women's One Piece Swimsuit Halter Tummy...</td>\n",
       "      <td>&lt;b&gt;CUPSHE Intro&lt;/b&gt;&lt;br&gt; To inspire confidence ...</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   example_id                     query  query_id  product_id product_locale  \\\n",
       "0      193260  amazon.com/code activate      8677  B0066TUXU6             us   \n",
       "1      264260           babygirl anklet     12307  B07663TBC4             us   \n",
       "2     1311389                 max japan     66517  B06Y1CBVKT             us   \n",
       "3     1632982       pressure washer oil     83198  B004S67CUS             us   \n",
       "4      300511      beach hats for women     14164  B072BJPTKJ             us   \n",
       "\n",
       "   esci_label  small_version  large_version  \\\n",
       "0  Irrelevant              1              1   \n",
       "1  Irrelevant              1              1   \n",
       "2  Irrelevant              1              1   \n",
       "3  Irrelevant              0              1   \n",
       "4  Irrelevant              0              1   \n",
       "\n",
       "                                       product_title  \\\n",
       "0                                               Hulu   \n",
       "1  Goldenchen Fashion Jewelry 925 Silver Plated A...   \n",
       "2  Max Vaimo HD-11FLK Flat Clinch Stapler with 3 ...   \n",
       "3        SIMPSON Cleaning Pressure Washer Pump Guard   \n",
       "4  CUPSHE Women's One Piece Swimsuit Halter Tummy...   \n",
       "\n",
       "                                 product_description  ... ctx_title_len  \\\n",
       "0                                                     ...           1.0   \n",
       "1  Goldenchen is a fashion jewelry leader that eq...  ...          13.0   \n",
       "2                                                     ...          12.0   \n",
       "3                                                     ...           6.0   \n",
       "4  <b>CUPSHE Intro</b><br> To inspire confidence ...  ...          15.0   \n",
       "\n",
       "  ctx_desc_len ctx_bullet_len ctx_brand_in_title  ctx_color_in_title  \\\n",
       "0          0.0          122.0                  1                   0   \n",
       "1        102.0           70.0                  1                   1   \n",
       "2          0.0           39.0                  0                   0   \n",
       "3          0.0           33.0                  1                   0   \n",
       "4        271.0           80.0                  1                   1   \n",
       "\n",
       "  prod_count  prod_mean_rel  prod_max_rel  query_count  query_mean_rel  \n",
       "0        1.0            0.0           0.0          4.0            0.75  \n",
       "1        1.0            0.0           0.0          4.0            0.50  \n",
       "2        1.0            0.0           0.0          5.0            0.00  \n",
       "3        1.0            0.0           0.0          2.0            0.00  \n",
       "4        1.0            0.0           0.0          5.0            1.80  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.1 Unified product_text_clean\n",
    "def build_product_text_clean(df: pd.DataFrame) -> pd.Series:\n",
    "    parts = [\n",
    "        df[\"product_title\"],\n",
    "        df[\"product_description\"],\n",
    "        df[\"product_bullet_point\"],\n",
    "        df[\"product_brand\"],\n",
    "        df[\"product_color\"],\n",
    "    ]\n",
    "    return (\n",
    "        parts[0].fillna(\"\")\n",
    "        + \" [SEP] \" + parts[1].fillna(\"\")\n",
    "        + \" [SEP] \" + parts[2].fillna(\"\")\n",
    "        + \" [SEP] \" + parts[3].fillna(\"\")\n",
    "        + \" [SEP] \" + parts[4].fillna(\"\")\n",
    "    )\n",
    "\n",
    "esci_train[\"product_text_clean\"] = build_product_text_clean(esci_train)\n",
    "esci_test[\"product_text_clean\"]  = build_product_text_clean(esci_test)\n",
    "\n",
    "# 2.2 Explicit \"context\" features from metadata\n",
    "def add_context_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # --- Basic lengths ---\n",
    "    df[\"ctx_title_len\"]  = df[\"product_title\"].fillna(\"\").str.split().str.len().astype(\"float32\")\n",
    "    df[\"ctx_desc_len\"]   = df[\"product_description\"].fillna(\"\").str.split().str.len().astype(\"float32\")\n",
    "    df[\"ctx_bullet_len\"] = df[\"product_bullet_point\"].fillna(\"\").str.split().str.len().astype(\"float32\")\n",
    "\n",
    "    # Precompute lowercased strings\n",
    "    title_lower = df[\"product_title\"].fillna(\"\").str.lower()\n",
    "    brand_lower = df[\"product_brand\"].fillna(\"\").str.lower()\n",
    "    color_lower = df[\"product_color\"].fillna(\"\").str.lower()\n",
    "\n",
    "    # --- Brand in title (row-wise substring check) ---\n",
    "    df[\"ctx_brand_in_title\"] = [\n",
    "        int((b != \"\") and (b in t))\n",
    "        for b, t in zip(brand_lower, title_lower)\n",
    "    ]\n",
    "\n",
    "    # --- Color in title (row-wise substring check) ---\n",
    "    df[\"ctx_color_in_title\"] = [\n",
    "        int((c != \"\") and (c in t))\n",
    "        for c, t in zip(color_lower, title_lower)\n",
    "    ]\n",
    "\n",
    "    # Cast to int8 for compactness\n",
    "    df[\"ctx_brand_in_title\"] = df[\"ctx_brand_in_title\"].astype(\"int8\")\n",
    "    df[\"ctx_color_in_title\"] = df[\"ctx_color_in_title\"].astype(\"int8\")\n",
    "\n",
    "    return df\n",
    "\n",
    "esci_train = add_context_features(esci_train)\n",
    "esci_test  = add_context_features(esci_test)\n",
    "\n",
    "# 2.3 Popularity / behaviour proxies (within ESCI train)\n",
    "prod_stats = (\n",
    "    esci_train.groupby(\"product_id\")[\"relevance\"]\n",
    "    .agg(prod_count=\"size\", prod_mean_rel=\"mean\", prod_max_rel=\"max\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "query_stats = (\n",
    "    esci_train.groupby(\"query_id\")[\"relevance\"]\n",
    "    .agg(query_count=\"size\", query_mean_rel=\"mean\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "esci_train = esci_train.merge(prod_stats, on=\"product_id\", how=\"left\")\n",
    "esci_train = esci_train.merge(query_stats, on=\"query_id\", how=\"left\")\n",
    "\n",
    "# For test, merge stats computed from train only\n",
    "esci_test = esci_test.merge(prod_stats, on=\"product_id\", how=\"left\")\n",
    "esci_test = esci_test.merge(query_stats, on=\"query_id\", how=\"left\")\n",
    "\n",
    "# Fill NaNs for unseen products/queries in test\n",
    "pop_cols = [\"prod_count\", \"prod_mean_rel\", \"prod_max_rel\", \"query_count\", \"query_mean_rel\"]\n",
    "for col in pop_cols:\n",
    "    esci_train[col] = esci_train[col].fillna(0).astype(\"float32\")\n",
    "    esci_test[col]  = esci_test[col].fillna(0).astype(\"float32\")\n",
    "\n",
    "esci_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d0a9a",
   "metadata": {},
   "source": [
    "# 3. Train/Valid Split (Grouped by query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0625cb36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T13:00:22.599721Z",
     "iopub.status.busy": "2025-11-24T13:00:22.599548Z",
     "iopub.status.idle": "2025-11-24T13:00:22.666883Z",
     "shell.execute_reply": "2025-11-24T13:00:22.665965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train / Valid shapes: (107125, 26) (11727, 26)\n"
     ]
    }
   ],
   "source": [
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=RANDOM_STATE)\n",
    "train_idx, valid_idx = next(gss.split(esci_train, groups=esci_train[\"query_id\"]))\n",
    "\n",
    "train_df = esci_train.iloc[train_idx].reset_index(drop=True)\n",
    "valid_df = esci_train.iloc[valid_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Train / Valid shapes:\", train_df.shape, valid_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9caf80b",
   "metadata": {},
   "source": [
    "# 4. BM25 Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87dc3ee2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T13:00:22.668704Z",
     "iopub.status.busy": "2025-11-24T13:00:22.668537Z",
     "iopub.status.idle": "2025-11-24T13:00:29.844087Z",
     "shell.execute_reply": "2025-11-24T13:00:29.843131Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('B001PO29TA', 22.126202648051027), ('B07YBRY2RL', 16.25763903390984), ('B07CR7449N', 15.901836445679972), ('B092MQ8F2V', 15.486902814225328), ('B08JR2C7JC', 15.286855978338346)]\n"
     ]
    }
   ],
   "source": [
    "def build_bm25_corpus(df: pd.DataFrame):\n",
    "    # one doc per product_id\n",
    "    prod_group = df.groupby(\"product_id\")[\"product_text_clean\"].first()\n",
    "    product_ids = prod_group.index.to_list()\n",
    "    corpus = [doc.split() for doc in prod_group.values]\n",
    "    return product_ids, corpus\n",
    "\n",
    "bm25_product_ids, bm25_corpus = build_bm25_corpus(esci_train)\n",
    "bm25 = BM25Okapi(bm25_corpus)\n",
    "\n",
    "# Map product_id -> index in BM25 corpus\n",
    "prodid_to_idx = {pid: i for i, pid in enumerate(bm25_product_ids)}\n",
    "\n",
    "def bm25_candidates_for_query(query_text: str, top_k: int = 100):\n",
    "    tokenized = query_text.split()\n",
    "    scores = bm25.get_scores(tokenized)\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(bm25_product_ids[i], float(scores[i])) for i in top_idx]\n",
    "\n",
    "# Example sanity check\n",
    "print(bm25_candidates_for_query(\"bathroom fan without light\", top_k=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "025917ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T13:00:29.846119Z",
     "iopub.status.busy": "2025-11-24T13:00:29.845930Z",
     "iopub.status.idle": "2025-11-24T13:00:44.309051Z",
     "shell.execute_reply": "2025-11-24T13:00:44.308080Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved BM25 corpus to: esci_pipeline/artifacts_us/bm25_corpus.pkl\n"
     ]
    }
   ],
   "source": [
    "# ---- SAVE BM25 corpus (optional but useful) ----\n",
    "import pickle\n",
    "\n",
    "with open(PATH_BM25_PKL, \"wb\") as f:\n",
    "    pickle.dump((bm25_product_ids, bm25_corpus), f)\n",
    "\n",
    "print(\"Saved BM25 corpus to:\", PATH_BM25_PKL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb8dbe",
   "metadata": {},
   "source": [
    "# 5. Attach BM25 Scores & Ranks to train/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2439d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T13:00:44.311231Z",
     "iopub.status.busy": "2025-11-24T13:00:44.311008Z",
     "iopub.status.idle": "2025-11-24T14:54:07.407076Z",
     "shell.execute_reply": "2025-11-24T14:54:07.406360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>bm25_score</th>\n",
       "      <th>bm25_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8677</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12307</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66517</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83198</td>\n",
       "      <td>16.197333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  bm25_score  bm25_rank\n",
       "0      8677    0.000000          1\n",
       "1     12307    0.000000          2\n",
       "2     66517    0.000000          2\n",
       "3     83198   16.197333          1\n",
       "4     79687    0.000000          1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attach_bm25_scores(df_queries: pd.DataFrame, top_k: int = 100) -> pd.DataFrame:\n",
    "    df_queries = df_queries.copy()\n",
    "    rows = []\n",
    "\n",
    "    for qid, sub in df_queries.groupby(\"query_id\"):\n",
    "        q_text = sub[\"query\"].iloc[0]\n",
    "        cand = bm25_candidates_for_query(q_text, top_k=top_k)\n",
    "        cand_map = {pid: score for pid, score in cand}\n",
    "        for idx, row in sub.iterrows():\n",
    "            pid = row[\"product_id\"]\n",
    "            bm25_score = cand_map.get(pid, 0.0)\n",
    "            rows.append((idx, bm25_score))\n",
    "\n",
    "    bm25_scores = pd.DataFrame(rows, columns=[\"row_idx\", \"bm25_score\"]).set_index(\"row_idx\")\n",
    "    df_queries = df_queries.join(bm25_scores, how=\"left\")\n",
    "    df_queries[\"bm25_score\"] = df_queries[\"bm25_score\"].fillna(0.0)\n",
    "\n",
    "    # Rank within each query (higher score = better rank)\n",
    "    df_queries[\"bm25_rank\"] = (\n",
    "        df_queries.groupby(\"query_id\")[\"bm25_score\"]\n",
    "        .rank(ascending=False, method=\"first\")\n",
    "        .astype(\"int32\")\n",
    "    )\n",
    "    return df_queries\n",
    "\n",
    "train_df = attach_bm25_scores(train_df, top_k=100)\n",
    "valid_df = attach_bm25_scores(valid_df, top_k=100)\n",
    "\n",
    "train_df[[\"query_id\", \"bm25_score\", \"bm25_rank\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf01ee",
   "metadata": {},
   "source": [
    "# 6. GTE Multilingual Embeddings & Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "398aa527",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T14:54:07.409338Z",
     "iopub.status.busy": "2025-11-24T14:54:07.409172Z",
     "iopub.status.idle": "2025-11-24T15:09:09.996993Z",
     "shell.execute_reply": "2025-11-24T15:09:09.996350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-24 06:54:08,917 [INFO] Load pretrained SentenceTransformer: Alibaba-NLP/gte-multilingual-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Alibaba-NLP/gte-multilingual-base were not used when initializing NewModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved product embeddings to: esci_pipeline/artifacts_us/prod_embs.npy\n",
      "Saved product IDs to: esci_pipeline/artifacts_us/product_ids.npy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>gte_score</th>\n",
       "      <th>gte_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8677</td>\n",
       "      <td>0.515014</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12307</td>\n",
       "      <td>0.652051</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66517</td>\n",
       "      <td>0.623146</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83198</td>\n",
       "      <td>0.651029</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79687</td>\n",
       "      <td>0.486416</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  gte_score  gte_rank\n",
       "0      8677   0.515014         2\n",
       "1     12307   0.652051         3\n",
       "2     66517   0.623146         1\n",
       "3     83198   0.651029         2\n",
       "4     79687   0.486416         4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ---- CONFIG ----\n",
    "GTE_MODEL_NAME = \"Alibaba-NLP/gte-multilingual-base\"\n",
    "\n",
    "# Load model on correct device\n",
    "gte_model = SentenceTransformer(\n",
    "    GTE_MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    device=DEVICE,   # \"cuda\" or \"cpu\" from your earlier config\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_texts(texts, batch_size=64):\n",
    "    # SentenceTransformer handles tokenization + model + pooling internally\n",
    "    embs = gte_model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,    # already L2-normalized -> cosine via dot product\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "    return embs\n",
    "\n",
    "# 6.1 Precompute product embeddings (train universe)\n",
    "prod_group = esci_train.groupby(\"product_id\")[\"product_text_clean\"].first()\n",
    "product_ids = prod_group.index.to_list()\n",
    "prod_embs   = encode_texts(prod_group.tolist(), batch_size=64)\n",
    "\n",
    "prodid_to_emb = {pid: emb for pid, emb in zip(product_ids, prod_embs)}\n",
    "\n",
    "# ---- SAVE product embeddings + IDs ----\n",
    "np.save(PATH_PROD_EMBS, prod_embs)\n",
    "np.save(PATH_PROD_IDS, np.array(product_ids))\n",
    "\n",
    "print(\"Saved product embeddings to:\", PATH_PROD_EMBS)\n",
    "print(\"Saved product IDs to:\", PATH_PROD_IDS)\n",
    "\n",
    "\n",
    "# 6.2 Compute query embeddings for train/valid (correctly aligned)\n",
    "train_q = train_df.groupby(\"query_id\")[\"query\"].first().reset_index()\n",
    "valid_q = valid_df.groupby(\"query_id\")[\"query\"].first().reset_index()\n",
    "\n",
    "train_query_embs = encode_texts(train_q[\"query\"].tolist())\n",
    "valid_query_embs = encode_texts(valid_q[\"query\"].tolist())\n",
    "\n",
    "qid_to_emb_train = {\n",
    "    int(qid): emb for qid, emb in zip(train_q[\"query_id\"].tolist(), train_query_embs)\n",
    "}\n",
    "qid_to_emb_valid = {\n",
    "    int(qid): emb for qid, emb in zip(valid_q[\"query_id\"].tolist(), valid_query_embs)\n",
    "}\n",
    "\n",
    "def attach_gte_scores(df: pd.DataFrame, qid_to_emb: dict) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    scores = []\n",
    "    for idx, row in df.iterrows():\n",
    "        q_emb = qid_to_emb[int(row[\"query_id\"])]\n",
    "        p_emb = prodid_to_emb[row[\"product_id\"]]\n",
    "        score = float(np.dot(q_emb, p_emb))  # cosine because normalized\n",
    "        scores.append(score)\n",
    "\n",
    "    df[\"gte_score\"] = scores\n",
    "    df[\"gte_rank\"] = (\n",
    "        df.groupby(\"query_id\")[\"gte_score\"]\n",
    "        .rank(ascending=False, method=\"first\")\n",
    "        .astype(\"int32\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "train_df = attach_gte_scores(train_df, qid_to_emb_train)\n",
    "valid_df = attach_gte_scores(valid_df, qid_to_emb_valid)\n",
    "\n",
    "train_df[[\"query_id\", \"gte_score\", \"gte_rank\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cda78ef",
   "metadata": {},
   "source": [
    "# 7. RRF Fusion Baseline (BM25 + GTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a93b3e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T15:09:09.999043Z",
     "iopub.status.busy": "2025-11-24T15:09:09.998880Z",
     "iopub.status.idle": "2025-11-24T15:09:10.031824Z",
     "shell.execute_reply": "2025-11-24T15:09:10.031302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>rrf_score</th>\n",
       "      <th>rrf_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14164</td>\n",
       "      <td>0.031778</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45325</td>\n",
       "      <td>0.032018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52411</td>\n",
       "      <td>0.032522</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63949</td>\n",
       "      <td>0.032018</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100206</td>\n",
       "      <td>0.032266</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  rrf_score  rrf_rank\n",
       "0     14164   0.031778         2\n",
       "1     45325   0.032018         2\n",
       "2     52411   0.032522         1\n",
       "3     63949   0.032018         2\n",
       "4    100206   0.032266         2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rrf_fusion(bm25_rank, gte_rank, c: float = 60.0):\n",
    "    return 1.0 / (c + bm25_rank) + 1.0 / (c + gte_rank)\n",
    "\n",
    "for df in (train_df, valid_df):\n",
    "    df[\"rrf_score\"] = rrf_fusion(df[\"bm25_rank\"], df[\"gte_rank\"])\n",
    "    df[\"rrf_rank\"]  = (\n",
    "        df.groupby(\"query_id\")[\"rrf_score\"]\n",
    "        .rank(ascending=False, method=\"first\")\n",
    "        .astype(\"int32\")\n",
    "    )\n",
    "\n",
    "valid_df[[\"query_id\", \"rrf_score\", \"rrf_rank\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46437662",
   "metadata": {},
   "source": [
    "# 8. MMR Baseline (Using GTE embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2848497f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T15:09:10.033464Z",
     "iopub.status.busy": "2025-11-24T15:09:10.033312Z",
     "iopub.status.idle": "2025-11-24T15:10:32.153029Z",
     "shell.execute_reply": "2025-11-24T15:10:32.152418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>mmr_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14164</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45325</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52411</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63949</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100206</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  mmr_rank\n",
       "0     14164         4\n",
       "1     45325         4\n",
       "2     52411         2\n",
       "3     63949         4\n",
       "4    100206         3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mmr_rerank(df_q: pd.DataFrame, lambda_: float = 0.7, top_k: int = 20) -> pd.DataFrame:\n",
    "    df_q = df_q.copy()\n",
    "    cand_idx = df_q.index.to_list()\n",
    "    selected = []\n",
    "    remaining = cand_idx.copy()\n",
    "\n",
    "    # Pre-collect product embeddings\n",
    "    pid_to_emb_local = {\n",
    "        pid: prodid_to_emb[pid] for pid in df_q[\"product_id\"].unique()\n",
    "    }\n",
    "\n",
    "    while remaining and len(selected) < top_k:\n",
    "        best_i = None\n",
    "        best_score = -1e9\n",
    "        for i in remaining:\n",
    "            row = df_q.loc[i]\n",
    "            rel = float(row[\"gte_score\"])\n",
    "            if not selected:\n",
    "                score = rel\n",
    "            else:\n",
    "                emb_i = pid_to_emb_local[row[\"product_id\"]]\n",
    "                max_sim = max(\n",
    "                    float(np.dot(emb_i, pid_to_emb_local[df_q.loc[j, \"product_id\"]]))\n",
    "                    for j in selected\n",
    "                )\n",
    "                score = lambda_ * rel - (1.0 - lambda_) * max_sim\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_i = i\n",
    "        selected.append(best_i)\n",
    "        remaining.remove(best_i)\n",
    "\n",
    "    # Build rank series for selected items\n",
    "    mmr_order = pd.Series(range(1, len(selected) + 1), index=selected)\n",
    "    df_q[\"mmr_rank\"] = (\n",
    "        df_q.index.to_series().map(mmr_order).fillna(len(df_q) + 1).astype(\"int32\")\n",
    "    )\n",
    "    return df_q\n",
    "\n",
    "def apply_mmr(df: pd.DataFrame, lambda_: float = 0.7, top_k: int = 20) -> pd.DataFrame:\n",
    "    parts = []\n",
    "    for qid, sub in df.groupby(\"query_id\"):\n",
    "        parts.append(mmr_rerank(sub, lambda_=lambda_, top_k=top_k))\n",
    "    return pd.concat(parts, axis=0).sort_index()\n",
    "\n",
    "train_df = apply_mmr(train_df, lambda_=0.7, top_k=20)\n",
    "valid_df = apply_mmr(valid_df, lambda_=0.7, top_k=20)\n",
    "\n",
    "# Convert MMR ranks into scores for evaluation\n",
    "for df in (train_df, valid_df):\n",
    "    # Lower rank = better item => use negative rank as score\n",
    "    df[\"mmr_score\"] = -df[\"mmr_rank\"].astype(\"float32\")\n",
    "\n",
    "\n",
    "valid_df[[\"query_id\", \"mmr_rank\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4d844",
   "metadata": {},
   "source": [
    "# 9. LambdaMART (Context-Aware LTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a66bc681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T15:10:32.155048Z",
     "iopub.status.busy": "2025-11-24T15:10:32.154894Z",
     "iopub.status.idle": "2025-11-24T15:11:29.789176Z",
     "shell.execute_reply": "2025-11-24T15:11:29.788614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LightGBM LambdaMART model to: esci_pipeline/artifacts_us/ltr_model_us.txt\n",
      "Saved prepared train_df to: esci_pipeline/artifacts_us/train_df_prepared.parquet\n",
      "Saved prepared valid_df to: esci_pipeline/artifacts_us/valid_df_prepared.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>ltr_score</th>\n",
       "      <th>ltr_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>1.116704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>-1.104282</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>-1.104282</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>-1.104282</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>0.802870</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  ltr_score  ltr_rank\n",
       "0         8   1.116704         1\n",
       "1        70  -1.104282         3\n",
       "2        70  -1.104282         4\n",
       "3        70  -1.104282         5\n",
       "4        70   0.802870         1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: sort by query_id so LightGBM group sizes match row order\n",
    "train_df = train_df.sort_values(\"query_id\").reset_index(drop=True)\n",
    "valid_df = valid_df.sort_values(\"query_id\").reset_index(drop=True)\n",
    "\n",
    "ltr_features = [\n",
    "    \"bm25_score\",\n",
    "    \"gte_score\",\n",
    "    \"ctx_title_len\",\n",
    "    \"ctx_desc_len\",\n",
    "    \"ctx_bullet_len\",\n",
    "    \"ctx_brand_in_title\",\n",
    "    \"ctx_color_in_title\",\n",
    "    \"prod_count\",\n",
    "    \"prod_mean_rel\",\n",
    "    \"prod_max_rel\",\n",
    "    \"query_count\",\n",
    "    \"query_mean_rel\",\n",
    "]\n",
    "\n",
    "X_train = train_df[ltr_features].astype(\"float32\")\n",
    "y_train = train_df[\"relevance\"].astype(\"float32\")\n",
    "group_train = train_df.groupby(\"query_id\").size().to_list()\n",
    "\n",
    "X_valid = valid_df[ltr_features].astype(\"float32\")\n",
    "y_valid = valid_df[\"relevance\"].astype(\"float32\")\n",
    "group_valid = valid_df.groupby(\"query_id\").size().to_list()\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train, group=group_train)\n",
    "lgb_valid = lgb.Dataset(X_valid, label=y_valid, group=group_valid, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"ndcg\",\n",
    "    \"ndcg_eval_at\": [10],\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 63,\n",
    "    \"min_data_in_leaf\": 50,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"verbosity\": -1,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "}\n",
    "\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=50, verbose=False),\n",
    "]\n",
    "\n",
    "lgbm_model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_train, lgb_valid],\n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    num_boost_round=300,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "\n",
    "# ---- SAVE LTR model ----\n",
    "lgbm_model.save_model(str(PATH_LGB_MODEL))\n",
    "print(\"Saved LightGBM LambdaMART model to:\", PATH_LGB_MODEL)\n",
    "\n",
    "# 9.1 Predict LTR scores\n",
    "valid_df[\"ltr_score\"] = lgbm_model.predict(\n",
    "    X_valid,\n",
    "    num_iteration=lgbm_model.best_iteration\n",
    ")\n",
    "valid_df[\"ltr_rank\"]  = (\n",
    "    valid_df.groupby(\"query_id\")[\"ltr_score\"]\n",
    "    .rank(ascending=False, method=\"first\")\n",
    "    .astype(\"int32\")\n",
    ")\n",
    "\n",
    "# ---- SAVE prepared train/valid DataFrames ----\n",
    "train_df.to_parquet(PATH_TRAIN_PREP, index=False)\n",
    "valid_df.to_parquet(PATH_VALID_PREP, index=False)\n",
    "\n",
    "print(\"Saved prepared train_df to:\", PATH_TRAIN_PREP)\n",
    "print(\"Saved prepared valid_df to:\", PATH_VALID_PREP)\n",
    "\n",
    "\n",
    "valid_df[[\"query_id\", \"ltr_score\", \"ltr_rank\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b2d2f",
   "metadata": {},
   "source": [
    "# 10. Cross-Encoder (DistilBERT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b06e20a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T15:11:29.790894Z",
     "iopub.status.busy": "2025-11-24T15:11:29.790746Z",
     "iopub.status.idle": "2025-11-24T16:14:09.876843Z",
     "shell.execute_reply": "2025-11-24T16:14:09.876254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79389898f777404880b16ea885f21fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/107125 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f389426606482082a73d8e8b900b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11727 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-24 07:11:48,884 [WARNING] Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66960' max='66960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66960/66960 1:02:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.003000</td>\n",
       "      <td>1.052852</td>\n",
       "      <td>0.558370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.864300</td>\n",
       "      <td>1.057088</td>\n",
       "      <td>0.570905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.716400</td>\n",
       "      <td>1.164926</td>\n",
       "      <td>0.559734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.556300</td>\n",
       "      <td>1.276992</td>\n",
       "      <td>0.559563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.448700</td>\n",
       "      <td>1.435516</td>\n",
       "      <td>0.555726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.360100</td>\n",
       "      <td>1.599343</td>\n",
       "      <td>0.554021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.281000</td>\n",
       "      <td>1.834243</td>\n",
       "      <td>0.555044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.223500</td>\n",
       "      <td>2.241846</td>\n",
       "      <td>0.546175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.154600</td>\n",
       "      <td>2.501801</td>\n",
       "      <td>0.552230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.169800</td>\n",
       "      <td>2.727857</td>\n",
       "      <td>0.550780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_hf_dataset(df: pd.DataFrame) -> Dataset:\n",
    "    tmp = df[[\"query\", \"product_text_clean\", \"relevance\"]].rename(\n",
    "        columns={\"relevance\": \"label\"}\n",
    "    )\n",
    "    # preserve_index=False avoids keeping the Pandas index as a separate column\n",
    "    return Dataset.from_pandas(tmp, preserve_index=False)\n",
    "\n",
    "train_hf = make_hf_dataset(train_df)\n",
    "valid_hf = make_hf_dataset(valid_df)\n",
    "\n",
    "tokenizer_x = AutoTokenizer.from_pretrained(XENC_MODEL_NAME)\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    enc = tokenizer_x(\n",
    "        batch[\"query\"],\n",
    "        batch[\"product_text_clean\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    enc[\"labels\"] = batch[\"label\"]\n",
    "    return enc\n",
    "\n",
    "train_tok = train_hf.map(tokenize_batch, batched=True)\n",
    "valid_tok = valid_hf.map(tokenize_batch, batched=True)\n",
    "\n",
    "train_tok = train_tok.remove_columns([\"query\", \"product_text_clean\"])\n",
    "valid_tok = valid_tok.remove_columns([\"query\", \"product_text_clean\"])\n",
    "\n",
    "train_tok.set_format(\"torch\")\n",
    "valid_tok.set_format(\"torch\")\n",
    "\n",
    "# 10.1 Model (4-class classification)\n",
    "num_labels = 4\n",
    "xenc_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    XENC_MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"single_label_classification\",\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models/esci_us_xenc\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=200,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = (preds == labels).mean()\n",
    "    return {\"accuracy\": float(acc)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=xenc_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=valid_tok,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 10.2 Getting CE scores for valid_df after training:\n",
    "preds = trainer.predict(valid_tok)\n",
    "logits = preds.predictions\n",
    "probs  = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "valid_df[\"ce_score\"] = probs[:, 3] + 0.5 * probs[:, 2]  # weight Exact/Substitute\n",
    "valid_df[\"ce_rank\"]  = (\n",
    "    valid_df.groupby(\"query_id\")[\"ce_score\"]\n",
    "    .rank(ascending=False, method=\"first\")\n",
    "    .astype(\"int32\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879891c",
   "metadata": {},
   "source": [
    "# 11. Evaluation (nDCG@K, P@K, gAUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1032107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-24T16:14:09.879250Z",
     "iopub.status.busy": "2025-11-24T16:14:09.879084Z",
     "iopub.status.idle": "2025-11-24T16:14:19.472337Z",
     "shell.execute_reply": "2025-11-24T16:14:19.471562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bm25_score {'nDCG@10': 0.8234731674889765, 'P@10': 0.7967233652343497, 'gAUC': 0.5792894990043588}\n",
      "gte_score {'nDCG@10': 0.8401853983140759, 'P@10': 0.7970691748356517, 'gAUC': 0.7507843379776473}\n",
      "rrf_score {'nDCG@10': 0.81520136450779, 'P@10': 0.7966623400105906, 'gAUC': 0.5632964850187847}\n",
      "ltr_score {'nDCG@10': 0.8758506878745544, 'P@10': 0.7974963514019657, 'gAUC': 0.9650283057502457}\n",
      "mmr_score {'nDCG@10': 0.8398444855474029, 'P@10': 0.796804732199362, 'gAUC': 0.5977450099316454}\n",
      "ce_score {'nDCG@10': 0.8406037396049565, 'P@10': 0.7964996060805662, 'gAUC': 0.6405646118871844}\n"
     ]
    }
   ],
   "source": [
    "def dcg_at_k(rels, k: int):\n",
    "    rels = np.asarray(rels)[:k]\n",
    "    if rels.size == 0:\n",
    "        return 0.0\n",
    "    discounts = np.log2(np.arange(2, rels.size + 2))\n",
    "    return float(np.sum((2 ** rels - 1) / discounts))\n",
    "\n",
    "def ndcg_at_k(rels, k: int):\n",
    "    rels = np.asarray(rels)\n",
    "    best = np.sort(rels)[::-1]\n",
    "    best_dcg = dcg_at_k(best, k)\n",
    "    if best_dcg == 0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(rels, k) / best_dcg\n",
    "\n",
    "def eval_ranking(df: pd.DataFrame, score_col: str, k: int = 10):\n",
    "    ndcgs = []\n",
    "    precs = []\n",
    "    auc_labels = []\n",
    "    auc_scores = []\n",
    "\n",
    "    for qid, sub in df.groupby(\"query_id\"):\n",
    "        sub_sorted = sub.sort_values(score_col, ascending=False)\n",
    "        rels = sub_sorted[\"relevance\"].values\n",
    "\n",
    "        ndcgs.append(ndcg_at_k(rels, k))\n",
    "        precs.append((rels[:k] > 0).mean())  # non-zero relevance => positive\n",
    "\n",
    "        auc_labels.extend((rels > 0).astype(int).tolist())\n",
    "        auc_scores.extend(sub_sorted[score_col].tolist())\n",
    "\n",
    "    ndcg = float(np.mean(ndcgs))\n",
    "    prec = float(np.mean(precs))\n",
    "    try:\n",
    "        gauc = float(roc_auc_score(auc_labels, auc_scores))\n",
    "    except ValueError:\n",
    "        gauc = float(\"nan\")\n",
    "\n",
    "    return {\"nDCG@{}\".format(k): ndcg, \"P@{}\".format(k): prec, \"gAUC\": gauc}\n",
    "\n",
    "for col in [\"bm25_score\", \"gte_score\", \"rrf_score\", \"ltr_score\",\"mmr_score\",\"ce_score\"]:\n",
    "    metrics = eval_ranking(valid_df, col, k=10)\n",
    "    print(col, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0c0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0658c20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b14a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f64b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3de3b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027c210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d63e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895d5fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62c3458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bcf2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8e9fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3d733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0547b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271d043",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac8ff74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (TF)",
   "language": "python",
   "name": "tf-py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0351b46f118e4d8790d1332ce7c468dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_26e0c52ef7d24fe8a35820cf69322bf5",
       "placeholder": "​",
       "style": "IPY_MODEL_12a0871498194dceba0c235fd1296f42",
       "tabbable": null,
       "tooltip": null,
       "value": " 11727/11727 [00:01&lt;00:00, 8514.45 examples/s]"
      }
     },
     "068b1809fb7a43fcb547ae0dc1c7170d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f6cda8b69e34e678168668ca51f8026": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fb182697d6c6483296aabc724d787505",
       "placeholder": "​",
       "style": "IPY_MODEL_1ecbb707fd0a4656b645e7882874dda3",
       "tabbable": null,
       "tooltip": null,
       "value": "Map: 100%"
      }
     },
     "10f389426606482082a73d8e8b900b9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2c0a7156c28741d793fbf74062dc51a4",
        "IPY_MODEL_bff4a250ae454b88a4376b9862978806",
        "IPY_MODEL_0351b46f118e4d8790d1332ce7c468dc"
       ],
       "layout": "IPY_MODEL_b599cbe2130941c8946bd1921883aef0",
       "tabbable": null,
       "tooltip": null
      }
     },
     "12a0871498194dceba0c235fd1296f42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1ecbb707fd0a4656b645e7882874dda3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "26e0c52ef7d24fe8a35820cf69322bf5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2c0a7156c28741d793fbf74062dc51a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5a7bc663f68c4a0190a8a54560bd3f07",
       "placeholder": "​",
       "style": "IPY_MODEL_95ee3e9c3a2c48f5ab4ba1c40a90dc99",
       "tabbable": null,
       "tooltip": null,
       "value": "Map: 100%"
      }
     },
     "43cc7c052ede44558d450c91ace0ed8a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d82737fe073941d3b6b14adda31675dd",
       "max": 107125,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_753876caae4d44bd9e2da6b3310f08ff",
       "tabbable": null,
       "tooltip": null,
       "value": 107125
      }
     },
     "5a7bc663f68c4a0190a8a54560bd3f07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "628efbef2b2a47289aa6da38543ee157": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "753876caae4d44bd9e2da6b3310f08ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "79389898f777404880b16ea885f21fd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0f6cda8b69e34e678168668ca51f8026",
        "IPY_MODEL_43cc7c052ede44558d450c91ace0ed8a",
        "IPY_MODEL_7db0cabe37dd4a5bac8e4aa8785564ea"
       ],
       "layout": "IPY_MODEL_068b1809fb7a43fcb547ae0dc1c7170d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "7db0cabe37dd4a5bac8e4aa8785564ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_ae768090f4c8469c99f77464a5a344df",
       "placeholder": "​",
       "style": "IPY_MODEL_a71051c7a7b1466899dd9d4c6ac4b454",
       "tabbable": null,
       "tooltip": null,
       "value": " 107125/107125 [00:14&lt;00:00, 5964.30 examples/s]"
      }
     },
     "95ee3e9c3a2c48f5ab4ba1c40a90dc99": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a71051c7a7b1466899dd9d4c6ac4b454": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ae768090f4c8469c99f77464a5a344df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b599cbe2130941c8946bd1921883aef0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bff4a250ae454b88a4376b9862978806": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_628efbef2b2a47289aa6da38543ee157",
       "max": 11727,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d43fcd3af2ed4d40a529c0029bf9b1d9",
       "tabbable": null,
       "tooltip": null,
       "value": 11727
      }
     },
     "d43fcd3af2ed4d40a529c0029bf9b1d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d82737fe073941d3b6b14adda31675dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fb182697d6c6483296aabc724d787505": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
